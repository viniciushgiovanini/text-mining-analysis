{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AS02: Representação Textual\n",
    "\n",
    "**Aluno: Vinícius Henrique Giovanini**  \n",
    "**Matricula: 692225**  \n",
    "**Professor: Wladmir Cardoso Brandão**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pprint as pp\n",
    "import re\n",
    "import numpy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "newsgroup_train = fetch_20newsgroups(subset='train', shuffle=False, random_state=42)\n",
    "pp.pprint(list(newsgroup_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(newsgroup_train.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de Ajuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_numpy_array_para_txt(nome_arquivo, array):\n",
    "    numpy.savetxt(nome_arquivo, array, fmt='%s')\n",
    "\n",
    "def ler_txt_para_numpy_array(nome_arquivo):\n",
    "    lista = []\n",
    "\n",
    "    with open(nome_arquivo, 'r') as arquivo:\n",
    "        for linha in arquivo:\n",
    "            lista.append(linha.strip())\n",
    "\n",
    "    return numpy.array(lista)\n",
    "  \n",
    "def salvar_dict_arq(nome_arquivo,dict):\n",
    "  with open(nome_arquivo, \"w\") as arquivo:\n",
    "    json.dump(dict, arquivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(txt):\n",
    "  tokens = re.sub(\"[^\\w]\", \" \", txt).split()\n",
    "  tokens = [w.lower() for w in tokens]\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processamento\n",
    "Removendo caracteres especiais "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = list(newsgroup_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txts(txt):\n",
    "  tokens = re.sub(\"[^\\w]\", \" \", txt).split()\n",
    "  clean_txt = [w.lower() for w in tokens]\n",
    "  return \" \".join(clean_txt)\n",
    "\n",
    "# Limpando o dataset inteiro removendo caracteres especiais e colocand to lower case\n",
    "data_list_cleaned = []\n",
    "\n",
    "for line in data_list:\n",
    " data_list_cleaned.append(clean_txts(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 11314)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list), len(data_list_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "X_counts = vect.fit_transform(data_list_cleaned)\n",
    "vocabulario = vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130107"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "salvar_numpy_array_para_txt(\"../saidas/AS02/data_list_processada.txt\", vocabulario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS**: Professor rodei criei o vocabulario e rodei o metodo para gerar o one hot enconding para cada frase, porem a celula passou 83m rodando e não terminou de fazer de todas as frases, pois tem 11k de frases com cerca de 100k de palavras. Por conta disso rodei para poucas frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def embed_frase(frase):\n",
    "    palavras = frase.split()\n",
    "    \n",
    "    embedding_frase = numpy.zeros(len(vocabulario))\n",
    "    \n",
    "    for palavra in palavras:\n",
    "        if palavra in vocabulario:\n",
    "            indice = numpy.where(vocabulario == palavra)[0][0]\n",
    "            embedding_frase[indice] = 1\n",
    "    \n",
    "    return embedding_frase\n",
    "\n",
    "\n",
    "one_hot_encoding = {}\n",
    "\n",
    "for i, line in enumerate(data_list_cleaned):\n",
    "    one_hot_encoding[line] = embed_frase(line)\n",
    "    \n",
    "    if i == 50:\n",
    "      break\n",
    "    print(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salvar_dict_arq(\"../saidas/AS02/one_hot_encoding.txt\", one_hot_encoding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
