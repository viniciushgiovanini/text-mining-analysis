{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AS02: Representação Textual\n",
    "\n",
    "**Aluno: Vinícius Henrique Giovanini**  \n",
    "**Matricula: 692225**  \n",
    "**Professor: Wladmir Cardoso Brandão**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pprint as pp\n",
    "import re\n",
    "import numpy\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_maximo_de_instancias = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "newsgroup = fetch_20newsgroups(subset='all', shuffle=False, random_state=42)\n",
    "pp.pprint(list(newsgroup.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(newsgroup.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de Ajuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_numpy_array_para_txt(nome_arquivo, array):\n",
    "    numpy.savetxt(nome_arquivo, array, fmt='%s')\n",
    "\n",
    "\n",
    "def ler_txt_para_numpy_array(nome_arquivo):\n",
    "    lista = []\n",
    "\n",
    "    with open(nome_arquivo, 'r') as arquivo:\n",
    "        for linha in arquivo:\n",
    "            lista.append(linha.strip())\n",
    "\n",
    "    return numpy.array(lista)\n",
    "  \n",
    "def salvar_dict_arq(nome_arquivo, dict):\n",
    "    with open(nome_arquivo, \"w\") as arquivo:\n",
    "        for chave, valor in dict.items():\n",
    "            linha = json.dumps({chave: valor.tolist()}) + '\\n'\n",
    "            arquivo.write(linha)\n",
    "            \n",
    "def carregar_dict_arq(nome_arquivo):\n",
    "    dict_carregado = {}\n",
    "    with open(nome_arquivo, \"r\") as arquivo:\n",
    "        for linha in arquivo:\n",
    "            obj_json = json.loads(linha)\n",
    "            chave, valor = list(obj_json.items())[0]\n",
    "            valor = numpy.array(valor)\n",
    "            dict_carregado[chave] = valor\n",
    "    return dict_carregado\n",
    "  \n",
    "def salvar_numpy_arq(nome_arquivo, numpy_lista, max_elementos=100):\n",
    "    with open(nome_arquivo, 'w') as f:\n",
    "        for array in numpy_lista:\n",
    "            elementos = array[:max_elementos]  # Apenas os primeiros max_elementos elementos\n",
    "            f.write(\"numpy.array([\")\n",
    "            f.write(numpy.array2string(elementos, separator=', ', formatter={'int': lambda x: str(x)}))\n",
    "            if len(array) > max_elementos:\n",
    "                f.write(\", ...\")  # Adiciona \"...\" se houver mais elementos no array\n",
    "            f.write(\"])\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(txt):\n",
    "  tokens = re.sub(\"[^\\w]\", \" \", txt).split()\n",
    "  tokens = [w.lower() for w in tokens]\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processamento\n",
    "Removendo caracteres especiais "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = list(newsgroup.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txts(txt):\n",
    "  tokens = re.sub(\"[^\\w]\", \" \", txt).split()\n",
    "  clean_txt = [w.lower() for w in tokens]\n",
    "  clean_txt.append(\"\\n\")\n",
    "  return \" \".join(clean_txt)\n",
    "\n",
    "# Limpando o dataset inteiro removendo caracteres especiais e colocand to lower case\n",
    "data_list_cleaned = []\n",
    "\n",
    "for line in data_list:\n",
    " data_list_cleaned.append(clean_txts(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 18846)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list), len(data_list_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vect = CountVectorizer()\n",
    "X_counts = vect.fit_transform(data_list_cleaned)\n",
    "vocabulario = vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173762"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "salvar_numpy_array_para_txt(\"../saidas/AS02/data_list_processada.txt\", vocabulario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### IMPORTANTE\n",
    "\n",
    "**OBS**: Rodei o one hot encoding em cima do dataset pre processado, porém tive que limitar o número max de features pois isso estava causando estouro de memória, na saída no txt para não ficar muito pesado para abrir limitei em 500 elementos do array para mostrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o vetorizador\n",
    "vectorizer = CountVectorizer(binary=True, max_features=numero_maximo_de_instancias)\n",
    "# Ajustar o vetorizador aos textos\n",
    "vectorizer.fit(data_list_cleaned)\n",
    "# Transformar os textos em vetores de one-hot encoding\n",
    "one_hot_encoding = vectorizer.transform(data_list_cleaned).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "salvar_numpy_arq(\"../saidas/AS02/20News_01.txt\", one_hot_encoding, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=numero_maximo_de_instancias)\n",
    "count_vectors_resp = count_vectorizer.fit_transform(data_list_cleaned)\n",
    "count_vectors_array = count_vectors_resp.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(count_vectors_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "salvar_numpy_arq(\"../saidas/AS02/20News_02.txt\", count_vectors_array, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=numero_maximo_de_instancias)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(data_list_cleaned).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "salvar_numpy_arq(\"../saidas/AS02/20News_03.txt\", tfidf_vectors, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams (2-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "count_vectors = count_vectorizer.fit_transform(data_list_cleaned)\n",
    "\n",
    "count_vectors_array = count_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "salvar_numpy_arq(\"../saidas/AS02/20News_04.txt\", tfidf_vectors, 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
